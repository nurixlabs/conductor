{
  "groups": [
    {
      "name": "conductor.workflow-status-listener.archival",
      "type": "com.netflix.conductor.contribs.listener.archive.ArchivingWorkflowListenerProperties",
      "sourceType": "com.netflix.conductor.contribs.listener.archive.ArchivingWorkflowListenerProperties"
    },
    {
      "name": "conductor.workflow-status-listener.queue-publisher",
      "type": "com.netflix.conductor.contribs.listener.conductorqueue.ConductorQueueStatusPublisherProperties",
      "sourceType": "com.netflix.conductor.contribs.listener.conductorqueue.ConductorQueueStatusPublisherProperties"
    }
  ],
  "properties": [
    {
      "name": "conductor.default-event-queue.type",
      "type": "java.lang.String",
      "description": "The default event queue type to listen on for the WAIT task."
    },
    {
      "name": "conductor.event-queues.amqp.enabled",
      "type": "java.lang.Boolean",
      "description": "Enable the use of RabbitMQ implementation to provide queues for consuming events.",
      "sourceType": "com.netflix.conductor.contribs.queue.amqp.config.AMQPEventQueueConfiguration"
    },
    {
      "name": "conductor.event-queues.nats-stream.enabled",
      "type": "java.lang.Boolean",
      "description": "Enable the use of NATS Streaming implementation to provide queues for consuming events.",
      "sourceType": "com.netflix.conductor.contribs.queue.nats.config.NATSStreamConfiguration"
    },
    {
      "name": "conductor.event-queues.nats.enabled",
      "type": "java.lang.Boolean",
      "description": "Enable the use of NATS implementation to provide queues for consuming events.",
      "sourceType": "com.netflix.conductor.contribs.queue.nats.config.NATSConfiguration"
    },
    {
      "name": "conductor.event-queues.sqs.enabled",
      "type": "java.lang.Boolean",
      "description": "Enable the use of AWS SQS implementation to provide queues for consuming events.",
      "sourceType": "com.netflix.conductor.contribs.queue.sqs.config.SQSEventQueueConfiguration"
    },
    {
      "name": "conductor.metrics-logger.reportPeriodSeconds",
      "type": "java.lang.Long",
      "description": "The interval (in seconds) at which the metrics will be reported into the log stream by the metrics-logger."
    },
    {
      "name": "conductor.tasks.http.connectTimeout",
      "type": "java.lang.Integer",
      "description": "The connection timeout of the underlying HttpClient used by the HTTP task."
    },
    {
      "name": "conductor.tasks.http.readTimeout",
      "type": "java.lang.Integer",
      "description": "The read timeout of the underlying HttpClient used by the HTTP task."
    },
    {
      "name": "conductor.tasks.kafka-publish.cacheSize",
      "type": "java.lang.Integer",
      "description": "The maximum number of entries permitted in the in-memory cache used by the KAFKA_PUBLISH task."
    },
    {
      "name": "conductor.tasks.kafka-publish.cacheTimeMs",
      "type": "java.lang.Integer",
      "description": "The duration after which a cached entry will be removed from the in-memory cache used by the KAFKA_PUBLISH task."
    },
    {
      "name": "conductor.tasks.kafka-publish.maxBlockMs",
      "type": "java.lang.String",
      "description": "The max.block.ms value that the kafka producer is configured with in the KAFKA_PUBLISH task."
    },
    {
      "name": "conductor.tasks.kafka-publish.requestTimeoutMs",
      "type": "java.lang.String",
      "description": "The request.timeout.ms value that the kafka producer is configured with in the KAFKA_PUBLISH task."
    },
    {
      "name": "conductor.workflow-execution-lock.type",
      "type": "java.lang.String",
      "description": "The implementation of the workflow execution lock to be used.",
      "defaultValue": "noop_lock"
    },
    {
      "name": "conductor.workflow-status-listener.archival.delay-queue-worker-thread-count",
      "type": "java.lang.Integer",
      "description": "The number of threads to process the delay queue in workflow archival",
      "sourceType": "com.netflix.conductor.contribs.listener.archive.ArchivingWorkflowListenerProperties",
      "defaultValue": 5
    },
    {
      "name": "conductor.workflow-status-listener.archival.ttl-duration",
      "type": "java.time.Duration",
      "description": "The time to live in seconds for workflow archiving module. Currently, only RedisExecutionDAO supports this",
      "sourceType": "com.netflix.conductor.contribs.listener.archive.ArchivingWorkflowListenerProperties",
      "defaultValue": 0
    },
    {
      "name": "conductor.workflow-status-listener.archival.workflow-archival-type",
      "type": "com.netflix.conductor.contribs.listener.archive.ArchivingWorkflowListenerProperties$ArchivalType",
      "description": "Archival type that we need in place. By Default the value is default",
      "sourceType": "com.netflix.conductor.contribs.listener.archive.ArchivingWorkflowListenerProperties"
    },
    {
      "name": "conductor.workflow-status-listener.archival.workflow-archive-unsuccessful-only",
      "type": "java.lang.Boolean",
      "description": "Set this variable to true if you want to archive only the workflows that didn't succeed. When true, only unsuccessful workflows will be archived, while both successful and unsuccessful workflows will be deleted from the datastore. This helps manage storage costs on S3 and keeps only the failed workflows for debugging.",
      "sourceType": "com.netflix.conductor.contribs.listener.archive.ArchivingWorkflowListenerProperties",
      "defaultValue": false
    },
    {
      "name": "conductor.workflow-status-listener.archival.workflow-s3-archival-bucket-region",
      "type": "java.lang.String",
      "description": "region of the S3 bucket where we want to archive the workflow",
      "sourceType": "com.netflix.conductor.contribs.listener.archive.ArchivingWorkflowListenerProperties",
      "defaultValue": "us-east-1"
    },
    {
      "name": "conductor.workflow-status-listener.archival.workflow-s3-archival-default-bucket-name",
      "type": "java.lang.String",
      "description": "name of the S3 bucket where we want to archive the workflow",
      "sourceType": "com.netflix.conductor.contribs.listener.archive.ArchivingWorkflowListenerProperties",
      "defaultValue": ""
    },
    {
      "name": "conductor.workflow-status-listener.queue-publisher.failure-queue",
      "type": "java.lang.String",
      "sourceType": "com.netflix.conductor.contribs.listener.conductorqueue.ConductorQueueStatusPublisherProperties",
      "defaultValue": "_callbackFailureQueue"
    },
    {
      "name": "conductor.workflow-status-listener.queue-publisher.finalize-queue",
      "type": "java.lang.String",
      "sourceType": "com.netflix.conductor.contribs.listener.conductorqueue.ConductorQueueStatusPublisherProperties",
      "defaultValue": "_callbackFinalizeQueue"
    },
    {
      "name": "conductor.workflow-status-listener.queue-publisher.success-queue",
      "type": "java.lang.String",
      "sourceType": "com.netflix.conductor.contribs.listener.conductorqueue.ConductorQueueStatusPublisherProperties",
      "defaultValue": "_callbackSuccessQueue"
    },
    {
      "name": "conductor.workflow-status-listener.type",
      "type": "java.lang.String",
      "description": "The implementation of the workflow status listener to be used."
    }
  ],
  "hints": [
    {
      "name": "conductor.default-event-queue.type",
      "values": [
        {
          "value": "sqs",
          "description": "Use AWS SQS as the event queue to listen on for the WAIT task."
        },
        {
          "value": "amqp",
          "description": "Use RabbitMQ as the event queue to listen on for the WAIT task."
        },
        {
          "value": "nats_stream",
          "description": "Use NATS Stream as the event queue to listen on for the WAIT task."
        }
      ]
    },
    {
      "name": "conductor.workflow-execution-lock.type",
      "values": [
        {
          "value": "noop_lock",
          "description": "Use the no-op implementation as the lock provider."
        },
        {
          "value": "local_only",
          "description": "Use the local in-memory cache based implementation as the lock provider."
        },
        {
          "value": "redis",
          "description": "Use the redis-lock implementation as the lock provider."
        },
        {
          "value": "zookeeper",
          "description": "Use the zookeeper-lock implementation as the lock provider."
        }
      ]
    },
    {
      "name": "conductor.workflow-status-listener.type",
      "values": [
        {
          "value": "stub",
          "description": "Use the no-op implementation of the workflow status listener."
        },
        {
          "value": "archive",
          "description": "Use then archive implementation which immediately archives the workflow upon termination or completion as the workflow status listener."
        },
        {
          "value": "queue_publisher",
          "description": "Use the publisher implementation which publishes a message to the underlying queue implementation upon termination or completion as the workflow status listener."
        }
      ]
    }
  ]
}